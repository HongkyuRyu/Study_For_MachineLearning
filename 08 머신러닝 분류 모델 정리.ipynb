{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. K-Nearest Neighbor(KNN)\n",
    "\n",
    "- K값(거리)를 설정한 후, K거리 안에(**반지름이 k인 원을 그린다**) 들어온 기존 데이터(훈련데이터)의 타입(분류)이 다수인 쪽(**확률적으로**)으로 예측하는 기법\n",
    "- 장점: 단순하고 이해하기 쉽다.\n",
    "- 단점: 적절한 k값 설정의 어려움이 있다. 데이터와 feature의 수가 많아지면, 예측시간이 오래 걸린다.\n",
    "\n",
    "- 분류에도 쓰이지만, 군집화(Clustering)에서 더 많이 쓰이는 기법이다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Logistic Regression(로지스틱 회귀)\n",
    "- 회귀 모델이 아닌 분류(Classfication) 모델\n",
    "\n",
    "- Sigmoid function을 사용해서 분류 하는 모델\n",
    "- **y =  1/ 1 + e^(-Wx + b)** : **결과값을 0과 1사이의 값으로 조정**하여 반환"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. SVM과 SVC\n",
    "- SVM(Support Vector Machine)는 분류를 위해 분류의 경계가 되는 **경계선**을 작성하여, 분류를 실행하는 모델\n",
    "    - margin: 결정선과 가장 가까운 양 옆의 데이터 간의 거리\n",
    "    - 결정선과 가장 가까운 양 옆의 데이터를 Support Vector 라고 한다.\n",
    "    - **SVM은 margin을 최대화(Aim: Maximum Margin)하는 결정선(Decision Boundary)를 구하는 알고리즘**\n",
    "- sklearn에서 SVM을 SVC라는 명칭으로 관련 함수를 제공"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Decision Tree\n",
    "- feature와 데이터를 분석하여, 패턴을 찾아내고, 예측가능한 규칙들을 Yes/No의 항목으로 나무(Tree)처럼 조합해서 예측하는 기법"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 앙상블(Ensemble)\n",
    "- 기본적인 컨셉은 여러 개의 기본 모델을 생성한 후, 전체 모델의 예측값을 종합해서 하나의 최종 모델 또는 예측 값을 도출하는 방식\n",
    "    - ex) 0.4정도의 예측 정확도를 보여주는 모델 5개가 있다고 하면, 이들을 결합해서 0.8 정도의 예측정확도를 보여주겠다 (앙상블의 기본 전략)\n",
    "\n",
    "- 기본 지향점; Low variance, Low bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### bias와 Variance 정리\n",
    "\n",
    "- **bias가 높다면** -> 학습 데이터에 대해서 예측 정확도가 떨어지는 경우\n",
    "    - underfitting 이거나, 지나치게 단순한 모델을 사용한 경우\n",
    "- **variance가 높다면** -> 새로운 데이터셋에 대해(test데이터셋) 정확도가 현저히 떨어지는 경우\n",
    "    - overfitting 이거나, 지나치게 복잡한 모델을 사용한 경우\n",
    "- **variance가 높고, bias가 낮은 경우**\n",
    "    - overfitting이 되어, 조금만 데이터가 달라도 다른 예측치를 내놓는 경우\n",
    "- **variance가 낮고, bias가 높은 경우**\n",
    "    - underfitting이 되어, 잘못된 구역에 틀린 예측치를 놓는 경우\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 실제 모델 구성시\n",
    "1. 학습 데이터 예측 정확도를 계산\n",
    "    - bias 정도를 파악한다. (underfitting 된 데이터가 없는지 체크해보기)\n",
    "2. 테스트셋에서도 학습데이터와 유사한 정확도를 가지는지 본다.\n",
    "    - variance를 판단한다\n",
    "\n",
    "* bias가 높은 경우: 더 복잡하게 모델을 만들거나 최적화를 더 깊게 시킨다.\n",
    "* variance가 높은 경우: overfitting 된 경우\n",
    "    - 가장 좋은 방법은 데이터의 수를 늘리는 것이다.\n",
    "    - feature수를 줄이거나, regularization 기법을 사용하여 튜닝을 한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5-1) Ensemble: Bagging, Boosting\n",
    "\n",
    "- **Bagging** ex) Random Forest\n",
    "    - Reduce overfit\n",
    "    - Reduce variance\n",
    "    - Independent classifiers\n",
    "\n",
    "- **Boosting** ex) Gradient Boosting\n",
    "    - Overfit possibility\n",
    "    - Reduce bias mainly\n",
    "    - Sequential classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #FFB300;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">Bagging 알고리즘 정리</font><br>\n",
    "\n",
    "#### Ensemble 기법 1: Bagging\n",
    "\n",
    "- Bagging = Bootstrap Aggregating\n",
    "- What is Bootstrap?:\n",
    "    - 주어진 **샘플 데이터 셋에서, 중복을 허용하여 무작위로 임의의 샘플셋을 복수개**로 만들고, 이를 통해 분포, 신뢰구간 같은 통계값을 계산하여, **모집단** 또는 **주어진 샘플 데이터셋보다 큰 데이터에 대한 분포** 등을 **추정**하는 기법\n",
    "- Bagging은 학습 데이터에서 샘플 데이터를 **여러 번** Boostrap 방식으로 추출한 뒤,\n",
    "    1. 각 모델을 학습시킨 후, 각 모델의 결과를 합친다.(=aggregation), 최종 예측을 만든다.\n",
    "    2. 여러 모델(=weak learner)의 결과를 잘 조합해서 strong learner를 만든다.\n",
    "    3. 동일 데이터에서 여러 샘플 데이터를 만들 수 있으므로 **학습 데이터 수를 늘리는 효과**가 있다.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 3px solid #0CD6F2;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">정리</font><br>\n",
    "\n",
    "일반적으로 bagging 기법은 개별 모델을 하나로 합치기 때문에, 가중치가 개별 모델보다 일반화되므로, **variance를 감소**시키는 역할을 한다.(=**overfitting을 약화시킨다.**)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bagging기법1: Random Forest\n",
    "\n",
    "- 랜덤포레스트\n",
    "   1. bootstrap 방식으로 학습 데이터에서 **여러 개의** 샘플 데이터를 추출하고\n",
    "   2. 전체 feature 중 일부 feature만 **랜덤하게 선택**하여 여러개의 의사 결정 트리를 만들어\n",
    "   3. 각 tree들을 각 샘플데이터로 학습하여 각 tree의 조건을 샘플 데이터에 적합하게 만든다.\n",
    "   4. 테스트 데이터로 **각 tree별 예측을 수행**\n",
    "   5. **각 tree 별 예측**값을 투표(aggregation)을 통해 다수결로 최종 예측값을 결정\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bagging기법: Extra Tree\n",
    "\n",
    "- 익스트림 랜덤 트리(Extremly randomized Trees)\n",
    "- 랜덤 포레스트보다 훨씬 더 랜덤한 성격을 가진다.\n",
    "    - **샘플 데이터 수까지도 랜덤하게 결정**해서, 다양한 모델을 만들고, **분류 조건도 랜덤하게 분할**하여 무작위성을 증대시킨다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #FFB300;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">Boosting 알고리즘 정리</font><br>\n",
    "\n",
    "#### Ensemble 기법 2: Boosting\n",
    "\n",
    "- Boosting은 **전체 학습 데이터를 기반**으로 한다는 점에서, Bagging과 차이점을 보인다.\n",
    "- **전체 학습 데이터를 기반으로 weak learner**를 구성한다.\n",
    "- **예측이 잘못된 데이터에 가중치를 더 두어** 다음 예측을 수행하는 방식을 반복한다.\n",
    "\n",
    "   1. 각 데이터마다 가중치(sample weight)칼럼을 만든다. 최초에는 동일한 sample weight를 가지도록 한다.\n",
    "   2. 최초 weak learner를 통해, 오류데이티의 sample weight를 높인다.\n",
    "   3. 이를 통해, 다음 weak learner를 학습시킬 훈련데이터 셋에 해당 오류 데이터가 들어갈 확률을 높인다. (= 오류 데이터에 집중한다.)\n",
    "   4. 각 weak learner는 **오류율에 따라 모델별 가중치(weight)가 결정된다**\n",
    "   5. N개의 모델을 진행하여, **모델별 가중치(weight) x 각 weak learner의 합** 으로 최종 예측값을 결정한다.\n",
    "\n",
    "- 즉, **예측이 틀린 데이터에 대해 보다 예측값을 보정해가며** 모델 성능을 개선하는 방식이다.\n",
    "- 기본 boosting기법은 AdaBoost(Adaptive Boosting)기법이라고 한다.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 3px solid #0CD6F2;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">정리</font><br>\n",
    "\n",
    "일반적으로 boosting기법은 반복을 통해 **오류를 보정하기 때문에** **bias를 감소시키는 역할**을 한다.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 주요 Boosting 기법1: GBM\n",
    "- GBM(Gradient Boosting Machine)\n",
    "- 기본적인 Boosting Concept + Gradient Descent 함수 사용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gradient Boosting\n",
    "- Gradient Boosting\n",
    "    - 경사하강법을 사용해서, Boosting을 수행하는 방법\n",
    "    - Gradient Boosting은 **residual fitting** (잔차 줄여나가는 방식)으로 이해하면 된다.\n",
    "    - 잔차 (residual)\n",
    "        - **모집단**을 기반으로 특정 모델을 통해, 예측을 수행한 후, 실제값과 예측값의 차이를 **오차(error)** 라고 한다.\n",
    "        - **표본집단**을 기반으로 특정 모델을 통해, 예측을 수행한 후, 실제값과 예측값의 차이를 **잔차(residual)** 라고 한다.\n",
    "\n",
    "#### 경사하강법 정리\n",
    "- h(x) = y - F(x)\n",
    "    - y: 실제값, F(x)는 x에 대해 가중치 값을 부여해서 예측한 값, h(x)는 오류값\n",
    "    - 즉, h(x)를 최소화하는 방향으로 가중치 값을 업데이트 하는 방법\n",
    "    - learning rate와 손실함수(loss function)의 순간기울기로 가중치를 업데이트를 하는 방식으로 계산"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 3px solid #0CD6F2;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">Question</font><br>\n",
    "\n",
    "단순히 미분이 0이 되는 값을 찾으면 되지 않을까?\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #FFB300;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">Answer</font><br>\n",
    "\n",
    "1. 전체 평균 오류 값 - 실제 값 = 0 이 각 데이터에 대한 정확한 예측을 의미하는 것이 아니다.\n",
    "2. 실제 머신러닝 예측이 필요한 사례는 목적함수가 반드시 **미분 가능한 형태로 주어지는 것이** 아니다.\n",
    "3. 미분이 0이라고 해서 Global Minima가 아닐 수 있다. (Local Minima)일 수 있음\n",
    "\n",
    "- 따라서, 경사하강법을 적용해서 Local Minima를 찾으려고 하는 것\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #FFB300;\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#3f8dbf;\">Gradient Boosting알고리즘</font><br>\n",
    "\n",
    "\n",
    "- **잔차를 대상으로 예측**을 하여, **잔차를 줄여나가기 위해 경사하강법**을 사용\n",
    "\n",
    "1. y(i) = 1, 2, .... n 에 대하여 초기 예측값인 y(i)를 **훈련 데이터의 타겟 값(y_train)의 평균으로 선택**한다.\n",
    "2. 각 훈련 데이터의 타겟값(실제값 y) - 예측값(초기는 평균값) 으로 **잔차(residual)**를 구한다.\n",
    "3. 훈련 데이터의 feature값을 기반으로 잔차(residual)를 예측하는 의사결정트리를 구성 (모델1)\n",
    "4. 구성된 **의사결정트리를 기반으로 잔차를 구하고, 이를 예측값(초기는 평균값)에 더한다.**\n",
    "    - 단, 각 데이터의 예측값 + 잔차 = 데이터의 타겟 값이 되지만, 이는 훈련 데이터에만 overfit한 것이므로\n",
    "\n",
    "    - learning rate를 통해 아래와 같이 계산\n",
    "        - 예측 값 + learning rate x 잔차 (모델1)\n",
    "    - 위 예측값은 실제 타겟 값과 차이가 있으므로 또 다시 잔차가 계산됨\n",
    "        - 예측 값 + learning rate x 잔차 (모델1) + learning rate x 잔차 (모델 2)\n",
    "5. 결과적으로\n",
    "    - 예측 값 + learning rate x 잔차(모델1) + learning rate x 잔차(모델2) .... +\n",
    "    - **이때, 실제값 = 최초 예측 값 + 잔차(모델1)**\n",
    "        - 모델 2가 예측해야 하는 더 작은 잔차는 아래와 같다.\n",
    "                - 더 작은 잔차 = 실제 값 - (예측값 + learning rate x 잔차(모델1)\n",
    "        - 이를 근사하게 되면 최종 값 =(근사) 예측값 + 잔차\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 주요 Boosting 기법2: XGBoost(eXtra Gradient Boost)\n",
    "- 기존의 GBM기법은 overfitting 가능성이 높다.\n",
    "- XGBOOST는 GBM에 **regularization**을 추가하여 overfitting문제를 해결함.\n",
    "- 또한, GBM은 순차적으로 모델을 개선하는 방식(예측값 + 잔차를 1... n)임에 반해, XGBOOST는 **병렬 수행**이 가능하며, 성능을 개선한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 주요 Boosting 기법3: LightGBM\n",
    "- XGBOOST가 GBM에 비해서 학습시간이 빨라졌지만, 여전히 학습시간이 **상대적으로 느림**\n",
    "- LightGBM은 XGBoost보다 대용량 처리가 가능하고, **GPU**를 지원하여, 더 적은 자원 메모리를 사용하는 모델이다.\n",
    "- LightGBM과 XGBoost 차이\n",
    "    #### XGBoost vs LightGBM\n",
    "    - XGBoost와 기존 Boosting기법은 최대한 **균형 트리**를 구성해서, **트리의 level(depth)f를 최소화**하는데 중점을 둔다.\n",
    "        - 이에 따라, 균형트리를 유지하기 위한 별도의 노력이 필요하다.\n",
    "    - LightGBM은 **leaf 노드**에서 추가적인 분류(분할)을 통해 학습을 진행하기 때문에, 균형트리 유지를 위한 별도의 노력을 기울일 필요가 없다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
